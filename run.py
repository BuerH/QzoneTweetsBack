import requests
import json
import re
from lxml import etree
import csv
import time
import random
import demjson3
from params import *

# --- é…ç½®åŒºåŸŸ ---
MAX_RETRIES = 5
MIN_DELAY = 3.0
MAX_DELAY = 6.0
TIMEOUT = 15
CSV_FILE = 'data.csv'

# éšæœº User-Agent
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

type_dict = {'1': 'ç•™è¨€', '2': 'å›å¤', '3': 'ç‚¹èµ'}

def get_random_headers():
    new_headers = headers.copy() if 'headers' in globals() else {}
    new_headers.update({
        'User-Agent': random.choice(USER_AGENTS),
        'Referer': 'https://user.qzone.qq.com/',
        'Origin': 'https://user.qzone.qq.com',
    })
    return new_headers

def parse_qzone_json(text):
    """è§£æ dirty json"""
    match = re.search(r'_Callback\(([\s\S]*?)\);?$', text.strip())
    if match:
        text = match.group(1)
    else:
        text = text.strip().strip('()')
    
    text = text.replace('undefined', 'null')
    
    try:
        return demjson3.decode(text)
    except demjson3.JSONDecodeError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æš´åŠ›æ­£åˆ™æå– data éƒ¨åˆ†ï¼ˆä½œä¸ºæœ€åçš„æ‰‹æ®µï¼‰
        return None

def fetch_data_with_retry(offset):
    url = 'https://user.qzone.qq.com/proxy/domain/ic2.qzone.qq.com/cgi-bin/feeds/feeds2_html_pav_all'
    current_params = params.copy()
    current_params['offset'] = str(offset)
    
    for attempt in range(MAX_RETRIES):
        try:
            sleep_time = random.uniform(MIN_DELAY, MAX_DELAY)
            if attempt > 0: sleep_time *= 2
            time.sleep(sleep_time)
            
            resp = requests.get(url, params=current_params, cookies=cookies, headers=get_random_headers(), timeout=TIMEOUT)
            
            if resp.status_code == 200:
                if 'login' in resp.text and 'location' in resp.text:
                    print("âŒ Cookie å·²å¤±æ•ˆï¼ˆæ£€æµ‹åˆ°ç™»å½•è·³è½¬ï¼‰")
                    return None
                return resp.text
            elif resp.status_code == 403:
                print("âš ï¸ 403 Forbiddenï¼Œæš‚åœ 10 ç§’")
                time.sleep(10)
        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {e}")
    return None

# --- ä¸»ç¨‹åº ---

if __name__ == '__main__':
    # åˆå§‹åŒ– CSV
    with open(CSV_FILE, 'w', encoding='utf-8-sig', newline='') as wf:
        writer = csv.writer(wf)
        writer.writerow(['è¡Œä¸º', 'å‘èµ·äººæ˜µç§°', 'å‘èµ·äººqqå·', 'æ—¶é—´', 'å‘èµ·äººç©ºé—´', 'æœ‰æ•ˆå†…å®¹', 'htmlé¡µé¢'])

    page = 0
    empty_count = 0 
    
    # ã€æ–°å¢ã€‘ç”¨äºè®°å½•å·²ç»çˆ¬è¿‡çš„åŠ¨æ€ï¼Œé˜²æ­¢æ­»å¾ªç¯
    seen_keys = set()

    print("ğŸš€ å¼€å§‹æŠ“å–...")

    while True:
        print(f'\nğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page//10 + 1} é¡µ (Offset: {page})')
        
        raw_text = fetch_data_with_retry(page)
        if not raw_text: break
            
        try:
            js = parse_qzone_json(raw_text)
            if not js:
                print("âŒ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ")
                page += 10
                continue
            
            items = js.get('data')
            
            # æ™ºèƒ½å±‚çº§æ¢æµ‹
            if items is None:
                # ... (ä¿æŒåŸæ¥çš„ç©ºæ£€æŸ¥é€»è¾‘) ...
                empty_count += 1
                if empty_count >= 3: break
                page += 10; continue
            elif isinstance(items, dict):
                if 'data' in items and isinstance(items['data'], list): items = items['data']
                elif 'feeds' in items and isinstance(items['feeds'], list): items = items['feeds']
                else: items = []
            elif isinstance(items, str): items = []

            if not items or len(items) == 0:
                if empty_count >= 3:
                    print("ğŸ›‘ è¿ç»­å¤šæ¬¡æ— æ•°æ®ï¼Œç»“æŸæŠ“å–ã€‚")
                    break
                print("âš ï¸ å½“å‰é¡µæ— æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€é¡µ")
                empty_count += 1
                page += 10
                continue
                
            empty_count = 0 
            
            # ã€æ–°å¢ã€‘æœ¬é¡µæ–°æ•°æ®è®¡æ•°å™¨
            new_items_on_this_page = 0
            
            rows_to_write = []
            for i, item in enumerate(items):
                try:
                    if not isinstance(item, dict): continue

                    # è·å–å…³é”®å­—æ®µ
                    act_qq = item.get('uin', '')
                    dt = item.get('feedstime', '')
                    act_name = item.get('nickname', 'æœªçŸ¥')
                    
                    # ã€å…³é”®æ­¥éª¤ã€‘ç”Ÿæˆå”¯ä¸€æŒ‡çº¹ (QQå· + å‘å¸ƒæ—¶é—´)
                    # ä¹Ÿå¯ä»¥åŠ ä¸Š item.get('key') å¦‚æœå­˜åœ¨çš„è¯ï¼Œä½† QQå·+æ—¶é—´ é€šå¸¸è¶³å¤Ÿå”¯ä¸€
                    unique_key = f"{act_qq}_{dt}"
                    
                    # æ£€æŸ¥æ˜¯å¦é‡å¤
                    if unique_key in seen_keys:
                        # è¿™æ˜¯ä¸€ä¸ªé‡å¤çš„åŠ¨æ€ï¼Œè·³è¿‡å†™å…¥ï¼Œä¹Ÿä¸å¢åŠ æ–°æ•°æ®è®¡æ•°
                        continue
                    
                    # è®°å½•è¿™ä¸ªæ–°åŠ¨æ€
                    seen_keys.add(unique_key)
                    new_items_on_this_page += 1

                    # ... (åŸæœ¬çš„æ•°æ®æå–å’Œè§£æé€»è¾‘) ...
                    act_type = str(item.get('typeid', ''))
                    act_home = item.get('userHome', '')
                    html = item.get('html', '').strip()
                    extract_text = ''
                    comments_list = ''
                    
                    if html:
                        html_xpath = etree.HTML(html)
                        if html_xpath is not None:
                            txt_list = html_xpath.xpath('//div[contains(@class,"txt-box")]//text()')
                            extract_text = ' '.join([t.strip() for t in txt_list if t.strip()])
                            if type_dict.get(act_type) == 'å›å¤':
                                cmt_list = html_xpath.xpath('//div[@class="mod-comments"]//text()')
                                comments_list = '\n'.join([c.strip() for c in cmt_list if c.strip()])

                    act_type_name = type_dict.get(act_type, 'å…¶å®ƒ')
                    final_content = extract_text + (f"\n[è¯„è®º]\n{comments_list}" if comments_list else "")
                    
                    rows_to_write.append([act_type_name, act_name, act_qq, dt, '', final_content, html])
                    print(f"   â””â”€ {act_name}: {extract_text[:15]}...")

                except Exception as e:
                    print(f"   âš ï¸ å•æ¡é”™è¯¯: {e}")
                    continue

            # ã€æ–°å¢ã€‘åˆ¤æ–­æœ¬é¡µæ˜¯å¦å…¨æ˜¯é‡å¤æ•°æ®
            if new_items_on_this_page == 0:
                print("\nğŸ›‘ è§¦å‘å»é‡æœºåˆ¶ï¼šå½“å‰é¡µæ‰€æœ‰æ•°æ®éƒ½å·²å­˜åœ¨ã€‚")
                print("è¿™æ„å‘³ç€æœåŠ¡å™¨åœ¨é‡å¤è¿”å›æœ€åä¸€é¡µï¼Œæˆ–è€…å·²ç»æ²¡æœ‰æ–°å†…å®¹äº†ã€‚")
                print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼Œæ­£å¸¸é€€å‡ºã€‚")
                break
            else:
                print(f"âœ… æœ¬é¡µå†™å…¥ {new_items_on_this_page} æ¡æ–°æ•°æ® (è¿‡æ»¤æ‰ {len(items) - new_items_on_this_page} æ¡é‡å¤)")

            with open(CSV_FILE, 'a', encoding='utf-8-sig', newline='') as wf:
                writer = csv.writer(wf)
                writer.writerows(rows_to_write)
            
            # ç¿»é¡µ
            page += 10
            
            # éšæœºå»¶è¿Ÿ
            sleep_time = random.uniform(2, 4)
            print(f"   â³ ä¼‘æ¯ {sleep_time:.1f} ç§’...")
            time.sleep(sleep_time)
            
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            break

    print(f"ğŸ‰ ç»“æŸã€‚å…±æŠ“å– {len(seen_keys)} æ¡ä¸é‡å¤åŠ¨æ€ã€‚")
